{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers,optimizers,losses\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices(\"gPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(ds_train, _),infos=tfds.load(\n",
    "    name=\"mnist\",\n",
    "    split=[\"train\",\"test\"],\n",
    "    shuffle_files=True,\n",
    "    with_info=True ,  \n",
    "    as_supervised=False\n",
    ")\n",
    "\n",
    "def normalize_to_normal(x):\n",
    "    return 255*(x-1)/2\n",
    "\n",
    "def normalize_data(data):\n",
    "    return tf.cast(2*(data[\"image\"]/255)-1,tf.float32)\n",
    "\n",
    "ds_train=ds_train.map(normalize_data,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train=ds_train.cache()\n",
    "ds_train=ds_train.shuffle(infos.splits[\"train\"].num_examples)#60,000\n",
    "ds_train=ds_train.batch(batch_size=64)\n",
    "ds_train=ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(ds_train.take(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTranspose(layers.Layer):\n",
    "    def __init__(self,channels,filter_size,stride=2,padding=\"same\",name=None):\n",
    "        super(CNNTranspose,self).__init__(name=name)\n",
    "        self.cnt=layers.Conv2DTranspose(channels,filter_size,stride,padding,use_bias=False)\n",
    "        self.bn=layers.BatchNormalization()\n",
    "    def call(self,inputs,training=False):\n",
    "        x=self.cnt(inputs)\n",
    "        x=self.bn(x,training=training)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "\n",
    "class Generator(keras.Model):\n",
    "    seed_dim=128\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.linear=layers.Dense(7*7*256,name=\"linear\")\n",
    "        self.tcn1=CNNTranspose(128,5,stride=1,name='TPCNN_1')\n",
    "        self.tcn2=CNNTranspose(64,5,stride=2,name=\"TPCNN_2\")\n",
    "        self.output_layer=layers.Conv2DTranspose(1,3,2,\"same\",name='output')\n",
    "    def call(self,inputs,training=False):\n",
    "        x=self.linear(inputs)\n",
    "        x=layers.Reshape((7,7,256))(x)\n",
    "        x=self.tcn1(x,training=training)\n",
    "        x=self.tcn2(x,training=training)\n",
    "        x=self.output_layer(x)\n",
    "        return tf.keras.activations.tanh(x)\n",
    "    def architecture(self):\n",
    "        x=keras.Input((128))\n",
    "        model=tf.keras.Model(inputs=[x],outputs=self.call(x))\n",
    "        return model.summary()\n",
    "    \n",
    "generator=Generator()\n",
    "generator.architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(layers.Layer):\n",
    "    def __init__(self,channels,filter_size,name=None):\n",
    "        super(CNN,self).__init__(name=name)\n",
    "        self.cnn=layers.Conv2D(channels,filter_size,strides=2,padding=\"same\",use_bias=False)\n",
    "        self.bn=layers.BatchNormalization()\n",
    "    def call(self,inputs,training=False):\n",
    "        x=self.cnn(inputs)\n",
    "        x=self.bn(x,training=training)\n",
    "        return tf.nn.leaky_relu(x)\n",
    "\n",
    "\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.cnn1=layers.Conv2D(64,3,2,padding=\"same\",name=\"Conv_1\")\n",
    "        self.cnn2=CNN(128,5,name=\"Conv_2\")\n",
    "        self.cnn3=CNN(256,5,name=\"Conv_1\")\n",
    "        self.output_layer=layers.Dense(1,'sigmoid',name=\"output\")\n",
    "    def call(self,inputs,training=False):\n",
    "        x=self.cnn1(inputs)\n",
    "        x=tf.nn.leaky_relu(x)\n",
    "        x=self.cnn2(x,training=training)\n",
    "        #x=self.cnn3(x,training=training)\n",
    "        x=layers.Flatten()(x)\n",
    "        x=self.output_layer(x)\n",
    "        return x\n",
    "    def architecture(self):\n",
    "        x=keras.Input((28,28,1))\n",
    "        model=tf.keras.Model(inputs=[x],outputs=self.call(x))\n",
    "        return model.summary()\n",
    "\n",
    "\n",
    "discriminator=Discriminator()\n",
    "discriminator.architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function\n",
    "\n",
    "* loss function : log(D(x))+log(1-D(G(z)))\n",
    "* Optimizer : Adam\n",
    "* learning rate :3e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_opt=optimizers.Adam(1e-4)\n",
    "disc_opt=optimizers.Adam(1e-4)\n",
    "loss_fn=losses.BinaryCrossentropy()\n",
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "* the Discriminator try to maximize the loss function\n",
    "* the Generator try to minimize log(1-D(G(Z))) or maximize log(D(G(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for idx,real in enumerate(tqdm.tqdm(ds_train)):\n",
    "        bach_size=real.shape[0]\n",
    "        z=tf.random.normal([bach_size,Generator.seed_dim])  #random noise\n",
    "        \n",
    "        \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape : \n",
    "            fake=generator(z,training=True) #G(z)\n",
    "\n",
    "            real_loss=loss_fn(tf.ones(bach_size,1),discriminator(real,training=True)) #log(D(x)) \n",
    "            fake_loss=loss_fn(tf.zeros(bach_size,1),discriminator(fake,training=True))#log(1-D(G(Z)))\n",
    "\n",
    "            disc_loss=real_loss+fake_loss\n",
    "            gen_loss=loss_fn(tf.ones(bach_size,1),discriminator(fake))\n",
    "        \n",
    "        grads_gen=gen_tape.gradient(gen_loss,generator.trainable_variables)\n",
    "        gen_opt.apply_gradients(zip(grads_gen,generator.trainable_variables))\n",
    "        grads_disc=disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
    "        disc_opt.apply_gradients(zip(grads_disc,discriminator.trainable_variables))\n",
    "        generated_image=generator(z,training=False)\n",
    "        if idx % 100 ==0:\n",
    "            img=tf.keras.preprocessing.image.array_to_img(normalize_to_normal(generated_image[0]))\n",
    "            img.save(f\"generated_images/generated_img{epoch}_{idx}_.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3663913924b1daa3d8d3926798e99031b125700758b9d2e89fd61fec1c9e6b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
